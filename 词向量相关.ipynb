{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量 \n",
    "\n",
    "对于非bert类型的模型，一般都采用词向量。虽然我们可以在训练下游模型的时候随机初始化词向量并训练词向量，但使用在大规模数据集上预训练的词向量往往能极大地加速模型收敛速度。因此，一般都会先加载预训练好的词向量。\n",
    "\n",
    "### 从哪里获取预训练的词向量\n",
    "\n",
    "GitHub上开源的[预训练词向量](https://github.com/Embedding/Chinese-Word-Vectors)\n",
    "\n",
    "其词向量的维度为300维，提供在各个语料数据集上训练出来的词向量，同时也提供基于不同context feature（在skip-gram训练的过程中，基于target word预测上下文单词；此时如果将上下文单词的字也作为预测对象的话，就是基于word + charactre训练的了）。\n",
    "\n",
    "### 如何训练自己的词向量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T07:47:57.061351Z",
     "start_time": "2020-10-15T07:47:56.558170Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "def read(fp='new.txt'):\n",
    "    with open(fp) as fd: \n",
    "        contents = [list(i.strip()) for i in fd if i != ''] \n",
    "    return contents\n",
    "sentences = read()\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences, size=300, window=5, min_count=5, sg=1, workers=multiprocessing.cpu_count())\n",
    "\n",
    "model.wv.save_word2vec_format('cgh_trained.wv', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中，我们使用了`gensim.models.Word2Vec`来训练基于我们自己的语料的词向量。其中，`sentences`是一个二维的数组，每一个元素是一个document，每一个doc由list of word组成（对于英文单词是每一个单词，对于中文来说是分词之后的每一个词，对于训练中文基于字的情况，word指的是每一个字）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
